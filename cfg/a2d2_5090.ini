[DIRS]

data_folder= /home/lrm/workspace/A2D2/
metadata_path =/home/lrm/workspace/A2D2/cams_lidars.json
mapping_path = /home/lrm/workspace/segment_net/params/a2d2.json
logs= /home/lrm/workspace/segment_net/logs

[MODE]

#[train, resume, test]
mode= train

[SPLITS]

train_seqs= ["20180925_101535", "20180925_112730", "20180925_135056", 
             "20181008_095521", "20181016_082154", "20181016_125231", 
             "20181107_132730", "20181108_091945", "20181108_103155",
             "20181108_123750", "20181108_141609", "20181204_135952",
             "20181204_170238"]
             
val_seqs= ["20180807_145028", "20181107_133258", "20181107_133445",
           "20181108_084007"]

test_sets= ["20180810_142822", "20180925_124435", "20181107_132300",
            "20181204_154421"]

[DATA]

# urban datasets:              [bdd100k, a2d2]
# off-road datasets:           [rellis3d, rudg]
# urban and off-road datasets: [goose]
dataset = a2d2

#number of samples (-1, all)
num_samples=-1
num_samples_plot = 6
batch_size = 1

[TRAIN]

epochs = 1000
patience = 2
steps_minibatch_optimizer = 1

[CHECKPOINT]

steps_train = 10
save_best = true
save_last = true
steps_plot_train = 1000

[MODEL]

learning_rate = 0.0001
weight_decay = 0.00001
dropout = 0.2
grad_norm = 5.0

#[batch_norm, group_norm]
norm_fn = group_norm

[BACKBONE]

# convolution: [resnet18, mobilenetv3, efficientnetb0, deeplabv3_mobilenetv3]
# transformer: [mobilevit, deit3_small, efficientformer, levit, segformerb0, pitxs, sam2_hiera, tinyvit, fastvit, maxxvitv2]
# hybrid: [convnextv2]
type = sam2_hiera

fpn_out_channels= 256

#[sum, concat, weighted_sum, max_pool]
aggregate=max_pool

# resnet18 -> 4 weights
# mobilenetv3 -> 5 weights
# deeplabv3_mobilenetv3 -> 5 weights
# efficientnetb0 -> 4 weights
# deit3_small -> 5 weights
# mobilevit -> 6 weights
# efficientformer -> 4 weights
# segformerb0 -> 4 weights
# pitxs -> 4 weights
# levit -> 4 weights
layer_weights=[2., 1., 1., 2., 1., 1.]

dropout = 0.2

[ATTENTION]

#[none, spatial, query, class_channel, se_channel]
type = query

reduction_rate = 4

dropout = 0.05

[HEAD]

dropout=0.2

#[se_conv_interp, depthwise_nn, transformer]
type=depthwise_nn

num_blocks = 4

# optimize classifier for low latency:
# true: conv + interpolation
# false: interpolation + conv
opt_latency = true

num_head_transformer = 2

[LOSS]

# dice, focal_dice, 
# cross_entropy, focal_cross_entropy, 
# lovasz_softmax, boundary_dice,
# hausdorff_dt_dice
type = dice

alpha = 0.25
gamma = 2.0

# Unknown              -> 0
# Car                  -> 1
# Bicycle              -> 2
# Truck                -> 3
# Other vehicles       -> 4
# Pedestrian           -> 5
# Road                 -> 6
# Parking area         -> 7
# Sidewalk             -> 8
# Non-drivable street  -> 9
# Buildings            -> 10
# Grid structure       -> 11
# Sidebars             -> 12
# Nature object        -> 13
# Poles                -> 14
# Traffic signal       -> 15
# Solid line           -> 16
# Obstacles / trash    -> 17 
# Sky                  -> 18

# Class weights for the image to handle imbalanced distribution
class_weights = [2.76,
                 0.72,
                 4.02,
                 1.55,
                 4.08,
                 4.11,
                 0.20,
                 3.14,
                 1.09,
                 2.18,
                 0.44,
                 1.81,
                 1.42,
                 0.18,
                 2.40,
                 2.27,
                 1.72,
                 3.22,
                 0.20]

# value to multiply the loss by
loss_scale = 10.0

[IMAGE]

num_classes = 19

#[width, height] = [960, 604]
image_size = [320, 201]

resize = true


